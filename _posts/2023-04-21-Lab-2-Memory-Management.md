---
layout: post
read_time: true
show_date: true
Categories: JOS
title:  JOS Lab 2 - Memory Management
date:   2023-04-21 22:48:20 +0800
description: JOS Lab 2 - Memory Management
tags: [coding, jos, operation system, C]
author: Obsidian0215
github: obsidian0215/jos
---

# Lab 2: Memory Management

```c
running JOS: (1.1s) 
  Physical page allocator: OK 
  Page management: OK 
  Kernel page directory: OK 
  Page management 2: OK 
Score: 70/70
```

## Part 1——Physical Page Management

- Ex1: In the file kern/pmap.c, you must implement code for the following functions (probably in the order given).`boot_alloc(),mem_init(),page_init(),page_alloc(),page_free()`
    
    **In pmap.c, the following functions are implemented:**
    
    ***boot_alloc*：**
    
    ```c
    static void *
    boot_alloc(uint32_t n)
    {
    	static char *nextfree;	// virtual address of next byte of free memory
    	char *result;
    
    	// Initialize nextfree if this is the first time.
    	// 'end' is a magic symbol automatically generated by the linker,
    	// which points to the end of the kernel's bss segment:
    	// the first virtual address that the linker did *not* assign
    	// to any kernel code or global variables.
    	if (!nextfree) {
    		extern char end[];
    		nextfree = ROUNDUP((char *) end, PGSIZE);
    	}
    
    	// Allocate a chunk large enough to hold 'n' bytes, then update nextfree.
      // Make sure nextfree is kept aligned to a multiple of PGSIZE.
    	//cprintf("boot_alloc memory at %.8x\n", nextfree);
    	//cprintf("next memory at %.8x\n", ROUNDUP((char *)(nextfree+n),PGSIZE));
    	
    	result = nextfree;
    	if (n > 0)
    		nextfree = ROUNDUP((char *)(nextfree + n), PGSIZE);
    	
    	if (nextfree > (char *)0xf0400000){     //only 4MB memory mapped
    		panic("boot_alloc: out of memory error!\n");
    		nextfree = result;	//resume nextfree
    		return NULL;
    	}
    	return result;
    }
    ```
    
    **In *mem_init*:** 
    
    ```c
    // Allocate an array of npages 'struct PageInfo's and store it in 'pages'.
    // The kernel uses this array to keep track of physical pages: for
    // each physical page, there is a corresponding struct PageInfo in this
    // array.  'npages' is the number of physical pages in memory.  Use memset
    // to initialize all fields of each struct PageInfo to 0.
    pages = (struct Pageinfo *)boot_alloc(sizeof(struct PageInfo) * npages);
    memset(pages,0,sizeof(struct PageInfo) * npages);
    
    cprintf("npages: %d\n", npages); 	
    cprintf("npages_basemem: %d\n", npages_basemem); 	
    cprintf("pages start at: %.8x\n", pages);
    cprintf("pages end at: %.8x\n", ((char*)pages) + (sizeof(struct PageInfo) * npages));
    
    ...
    ```
    
    ***page_init*:** 
    
    ```c
    /*                    Low Physical Memory Address Layout
     *                     .                              .
     *                     .       Managable Space        .
     *                     .                              .
     *   ends 0x157000  -->+------------------------------+
     *                     |                              |
     *                     .   pages management array     .
     *                     .          (pages)             .
     *  pages 0x116000 ->  |       (kern_pgdir)           |
     *  pgdir 0x117000 ->  +------------------------------+
     *                     |        Kernel's here         |
     *    EXT 0x100000 ->  +------------------------------+
     *                     |          ~~IO Hole~~             |
     * BASEMEM 0xa0000 ->  +------------------------------+
     *                     |    Basic Managable Space     |
     *    KERNBASE ----->  +------------------------------+
     *  (0xF0000000)=phys addr 0x0
     */
    
    void page_init(void) {
    	//  1) Mark physical page 0 as in use.
    	//     This way we preserve the real-mode IDT and BIOS structures
     	//     in case we ever need them.  (Currently we don't, but...)
     	//  2) The rest of base memory, [PGSIZE, npages_basemem * PGSIZE) is free.
    	//  3) Then comes the IO hole [IOPHYSMEM, EXTPHYSMEM), which must never be allocated.
    	//  4) Then extended memory [EXTPHYSMEM, ...). Some of it is in use, some is free.
    	//     **The kernel starts at 0x100000 in physical memory.**
    	//     ***Pages* at 0x116000 are already in use for page tables and other data structures.**
    #define MARK_FREE(_i) do {\
        pages[_i].pp_ref = 0;\
        pages[_i].pp_link = page_free_list;\
    		page_free_list = &pages[_i];\
    } while(0)
    #define MARK_USE(_i) do {\
        pages[_i].pp_ref = 0;\
        pages[_i].pp_link = NULL;\
    } while(0)
    
    	  physaddr_t boot_alloc_end;
    		size_t i;
    
    	  page_free_list = NULL;
    	//jump over the gap between Base(IO) and Extended
    	//boot_alloc(0) returns "current" PG_aligned nextfree virt-addr
    	//last boot_alloc call allocated pages(=sizeofPageInfo*npages)
        boot_alloc_end = PADDR(boot_alloc(0));
        MARK_USE(0);
        for (i = 1; i < npages_basemem; ++i)
            MARK_FREE(i);
        for (i = IOPHYSMEM / PGSIZE; i < boot_alloc_end / PGSIZE; ++i)
            MARK_USE(i);
        for (i = boot_alloc_end / PGSIZE; i < npages; ++i)
            MARK_FREE(i);
    
    #undef MARK_USE
    #undef MARK_FREE
    }
    ```
    
    ***page_alloc*:**
    
    ```c
    struct PageInfo *
    page_alloc(int alloc_flags)
    {
    	if (page_free_list == NULL)
    		return 0;
    	struct PageInfo *target = page_free_list;
    	page_free_list = page_free_list->pp_link;	//next free page
    	target->pp_link = NULL;		//ensure target is out of free_list
    	if (alloc_flags & ALLOC_ZERO)
    		memset(page2kva(target),0,PGSIZE);		//init a page from kaddr(target)
    	return target;
    }
    ```
    
    ***page_free*:** 
    
    ```c
    void
    page_free(struct PageInfo *pp)
    {
    	if (pp->pp_ref || pp->pp_link)
    		panic("page_free: can't free a page which is currently in use!\n");
    	pp->pp_link = page_free_list;
    	page_free_list = pp;
    }
    ```
    

## Part 2——Virtual Memory

- ~~Ex2~~: Look at chapters 5 and 6 of the Intel 80386 Reference Manual and read the sections about page translation and page-based protection closely (5.2 and 6.4). While JOS uses the paging hardware for virtual memory and protection, segment translation and segment-based protection cannot be disabled on the x86.
    
    ```
               Selector  +--------------+         +-----------+
              ---------->|              |         |           |
                         | Segmentation |         |  Paging   |
    Software             |              |-------->|           |---------->  RAM
                Offset   |  Mechanism   |         | Mechanism |
              ---------->|              |         |           |
                         +--------------+         +-----------+
                Virtual                   Linear                Physical
    ```
    
    A C pointer is the "offset" component of the virtual address. In boot/boot.S, the GDT effectively **disabled segmentation** by **setting all segment base addresses to 0 and limits to 0xffffffff**. Hence the "selector" has no effect and **the linear address always equals the offset of the virtual address**. 
    
    **Knowledgements about memory translation are in:** 
    
    [x86 Architecture](https://www.notion.so/x86-Architecture-0ccb48ebe84148d3a87f0d2fc9bab5a5)
    
- **Virtual, Linear and Physical Addresses**
    
    Ex3: While GDB can only access QEMU's memory by virtual address, it's often useful to be able to inspect physical memory while setting up virtual memory. Review the QEMU [monitor commands](https://pdos.csail.mit.edu/6.828/2018/labguide.html#qemu) from the lab tools guide, especially the xp command, which lets you inspect physical memory. To access the QEMU monitor, press **Ctrl-a c** in the terminal (the same binding returns to the serial console).
    
    Use the **xp** command in the QEMU monitor and the **x** command in GDB to inspect memory at corresponding physical and virtual addresses and make sure you see the same data.
    
    **In QEMU monitor, *x(/fmt) addr* will print the guest virtual address starting at *addr*(like *x* in GDB), while *xp/fmt addr* will dump the guest physical address starting at *addr*. (In QEMU monitor, there are some commands such as *gpa2hpa/gpa2hva/gva2gpa addr* which can print corresponding host address to a guest address.)** 
    
    **When we run kernel to *page_init* and is exactly in page-linking loop, we watch the contents of *pages.***
    
    **In QEMU monitor, we can see that there are same contents in the virtual addresses and corresponding physical addresses (*pva2pga* shows the same address mapping)**
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled.png)
    
    **In GDB, the result is same as QEMU monitor(vaddr)**
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%201.png)
    
    Question: 
    
    1. Assuming that the following JOS kernel code is correct, what type should variable x have, uintptr_t or physaddr_t?
    
    ```c
    mystery_t x;
    char* value = return_a_pointer();
    *value = 10;
    x = (mystery_t) value; //uintptr_t——C pointer is a virtual address
    ```
    
    Unfortunately JOS kernel can't bypass virtual address translation and thus cannot directly load and store to physical addresses——JOS remaps all of physical memory starting from physical address 0 at virtual address 0xf0000000 to help the kernel read and write memory for which it knows just the physical address.**(use KADDR(pa))**
    
    The JOS kernel also sometimes needs to be able to find a physical address given the virtual address of the memory in which a kernel data structure is stored——Kernel global variables and memory allocated by `boot_alloc()` are in the region where the kernel was loaded, starting at 0xf0000000, the very region where we mapped all of physical memory.**(use PADDR(va))**
    
- **Referrence counting**
    
    In future labs, a single physical page may be mapped at multiple virtual addresses simultaneously (or in the address spaces of multiple environments). Keep a count of the number of references to each physical page in the pp_ref field of the struct PageInfo corresponding to the physical page. (zero count for a physical page means it can be freed) We'll also use it to keep track of the number of pointers we keep to the page directory pages and, in turn, the number of references the page directories have to page table pages.
    
    The page which page_alloc returns will always have a reference count of 0, so pp_ref should be incremented as soon as you've done something with the returned page. Sometimes this is handled by other functions (for example, page_insert) and sometimes the function calling page_alloc must do it directly.
    

### Page Table Management

- Ex4: In the file kern/pmap.c, you must implement code for the following functions. (a set of routines to manage page tables: to insert and remove linear-to-physical mappings, and to create page table pages when needed.) `pgdir_walk(), boot_map_region(), page_lookup(), page_remove(), page_insert()`
    
    ***pgdir_walk*:**
    
    ```c
    // Given 'pgdir', a pointer to a page directory, pgdir_walk returns
    // a pointer to the page table entry (PTE) for linear address 'va'.
    
    // The relevant page table page might not exist yet.
    // If this is true, and create == false, then pgdir_walk returns NULL.
    // Otherwise, pgdir_walk allocates a new page table page with page_alloc.
    //    - If the allocation fails, pgdir_walk returns NULL.
    //    - Otherwise, the new page's reference count is incremented,
    // 	the page is cleared,
    // 	and pgdir_walk returns a pointer into the new page table page.
    pte_t *
    pgdir_walk(pde_t *pgdir, const void *va, int create)
    {
    	// Fill this function in
    	pde_t *pde;
    	struct PageInfo * pi;
    
    	pde = &pgdir[PDX(va)];
    	if (!(*pde & PTE_P)){	//pde not present
    		if (!create || !(pi = page_alloc(ALLOC_ZERO)))	//won't create or fail to allocate a pagetable
    			return NULL;
    		*pde = page2pa(pi) | PTE_P | PTE_W | PTE_U | PTE_PWT | PTE_PCD | PTE_G | PTE_PS;
    		pi->pp_ref++;
    	}
    	pde = KADDR(PTE_ADDR(*pde));	//kva of the page table
    	return (pde + PTX(va));
    }
    ```
    
    ***boot_map_region*:**
    
    ```c
    // Map [va, va+size) of virtual address space to physical [pa, pa+size)
    // in the page table rooted at pgdir.  Size is a multiple of PGSIZE, and
    // va and pa are both page-aligned. permission bits perm|PTE_P for the entries.
    
    // This function is only intended to set up the "static" mappings above UTOP:
    // it should *not* change the pp_ref field on the mapped pages.(kernel use it)
    static void
    boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm)
    {
    	// Fill this function in
    	size_t i;
    	pte_t *pte;
    
    	if ((va & (PGSIZE - 1)) || (pa & (PGSIZE - 1)) || (size % PGSIZE))
    		panic("boot_map_region:non page-aligned");
    
    	for (i = 0; i < size; i+=PGSIZE){
    		if (!(pte = pgdir_walk(pgdir, (void*)(va + i), 1)))
    			panic("boot_map_region:failed to allocate a pagetable");
    		*pte = (pa + i) | perm | PTE_P;		//permissions in pte should be strict
    	}
    }
    ```
    
    ***page_lookup*:**
    
    ```c
    // Return the page mapped at virtual address 'va'. If pte_store is non-zero,
    // then we store in it the address of the pte for this page.  
    // This is nearly only allowed to be used by page_remove,
    // or to verify page permissions for syscall arguments.
    
    // Return NULL if there is no page mapped at 'va'.
    struct PageInfo *
    page_lookup(pde_t *pgdir, void *va, pte_t **pte_store)
    {
    	// Fill this function in
    	pte_t *pte = pgdir_walk(pgdir, va, 0);
    	if (!pte || !(*pte & PTE_P))
    		return NULL;
    	if (pte_store)
    		*pte_store = pte;
    	return pa2page(PTE_ADDR(*pte));
    }
    ```
    
    ***page_remove*:**
    
    ```c
    // Unmaps the physical page at virtual address 'va'.
    // If there is no physical page at that address, silently does nothing.
    // If such a PTE exists:
    //   - The ref count on the physical page should decrement.
    //   - The physical page should be freed if the refcount reaches 0.
    //   - The pg table entry corresponding to 'va' should be set to 0.
    //   - The TLB must be invalidated if you remove an entry from the page table.
    void
    page_remove(pde_t *pgdir, void *va)
    {
    	// Fill this function in
    	pte_t *pte;
    	struct PageInfo *pi;
    
    	pi = page_lookup(pgdir, va, &pte);
    	if (!pi)
    		return;
    	page_decref(pi);
    	tlb_invalidate(pgdir, va);
    	*pte = 0;
    }
    ```
    
    ***page_insert*:**
    
    ```c
    // Map the physical page 'pp' at virtual address 'va'.
    // The permissions (the low 12 bits) of the page table entry
    // should be set to 'perm|PTE_P'.
    //   - If there is already a page mapped at 'va', it should be page_remove()d.
    //   - If necessary, on demand, a page table should be allocated and inserted
    //     into 'pgdir'.
    //   - pp->pp_ref should be incremented if the insertion succeeds.
    //   - The TLB must be invalidated if a page was formerly present at 'va'.
    //   - RETURNS: 0 on success || -E_NO_MEM if page table couldn't be allocated
    
    // Corner-case attention: The same pp may be re-inserted
    // at the same virtual address in the same pgdir, 
    // **which can make a physical page both being page_freed and mapped in pgtable.**
    int
    page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm)
    {
    	// Fill this function in
    	pte_t *pte;
    	if (!(pte = pgdir_walk(pgdir, va, 1)))	//create on demand
    		return -E_NO_MEM;
    	pp->pp_ref++;	//inc ref_count beforehand to avoid 're-inserted' corner case
    	if (*pte & PTE_P)
    		page_remove(pgdir, va);		//delete previous page mapping
    	*pte = page2pa(pp) | PTE_P | perm;
    	return 0;
    }
    ```
    
    **Result: all mem-check pass.**
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%202.png)
    

## Part 3: Kernel Address Space

JOS divides the processor's 32-bit linear address space into two parts. User environments (processes) will have control over the layout and contents of the lower part, while the kernel always maintains complete control over the upper part. The dividing line is defined somewhat arbitrarily by the symbol ULIM in inc/memlayout.h, reserving approximately 256MB of virtual address space for the kernel, otherwise there would not be enough room in the kernel's virtual address space to map in a user environment below it at the same time.

- Permissions and Fault Isolation
    
    Since kernel and user memory are both present in each environment's address space, using permission bits in our x86 page tables allows user code access only to the user part of the address space. **The writable permission bit (PTE_W) affects both user and kernel code!**
    
    The user environment will have no permission to any of the memory above `ULIM`, while the kernel will be able to read and write this memory. 
    
    For the address range `[UTOP,ULIM)`, both the kernel and the user environment have the same permission: they can read but not write this address range. This range of address is used to expose certain kernel data structures read-only to the user environment. 
    
    The address space below `UTOP` is for the user environment to use: the user environment will set permissions for accessing this memory.
    

### Initializing the Kernel Address

- Ex5: set up the address space above `UTOP`: the kernel part of the address space.  Fill in the missing code in `mem_init()` after the call to `check_page()`. Your code should now pass the `check_kern_pgdir()` and `check_page_installed_pgdir()` checks. **(Use *boot_map_region*)**
    
    **In *pmap.c*:** 
    
    ```c
    // Map 'pages' read-only by the user at linear address UPAGES
    // Permissions:
    //    - the new image at UPAGES -- kernel R, user R
    //      (ie. perm = PTE_U | PTE_P)
    //    - struct pages itself -- kernel RW, user NONE
    boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U);
    
    // Use the physical memory that 'bootstack' refers to as the kernel
    // stack.  The kernel stack grows down from virtual address KSTACKTOP.
    // We consider the entire range from [KSTACKTOP-PTSIZE, KSTACKTOP)
    // to be the kernel stack, but break this into two pieces:
    //     * [KSTACKTOP-KSTKSIZE, KSTACKTOP) -- backed by physical memory
    //     * [KSTACKTOP-PTSIZE, KSTACKTOP-KSTKSIZE) -- not backed; so if
    //       the kernel overflows its stack, it will fault rather than
    //       overwrite memory.  Known as a "guard page".
    //     Permissions: kernel RW, user NONE
    boot_map_region(kern_pgdir, KSTACKTOP-KSTKSIZE, KSTKSIZE, PADDR(bootstacktop)-KSTKSIZE, PTE_W);
    
    // Map all of physical memory at KERNBASE.
    // Ie.  the VA range [KERNBASE, 2^32) should map to
    //      the PA range [0, 2^32 - KERNBASE)
    // We might not have 2^32 - KERNBASE bytes of physical memory, but
    // we just set up the mapping anyway.
    // Permissions: kernel RW, user NONE
    boot_map_region(kern_pgdir, KERNBASE, ~KERNBASE+1, 0, PTE_W);
    ```
    
    **Result:** 
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%203.png)
    
    Question: 
    
    1. What entries (rows) in the page directory have been filled in at this point? What addresses do they map and where do they point? In other words, fill out this table as much as possible:
        
        How much space overhead is there for managing memory, if we actually had the maximum amount of physical memory? How is this overhead broken down?
        
        | Entry | Base Virtual Address | Points to (logically) |
        | --- | --- | --- |
        | 1023 |  0xFFC00000 | Page table for top 4MB of phys memory |
        | 1022 |  0xFF800000 | Page table for top 8~4MB of phys memory |
        | ... |  |  |
        | 960 |  0xF0000000 | Page table for bottom 4MB of phys memory |
        | 959 | 0xEFC00000 | Page table that contains kernel stack |
        | ... |  |  |
        | 957 | 0xEFC40000 | Virtual page directory |
        | 956 | 0xEFC00000 | Page table mapping `pages` to user space |
        | ... |  |  |
        | 1 | 0x00400000 | \ |
        | 0 | 0x00000000 | \ |
    2. We have placed the kernel and user environment in the same address space. Why will user programs not be able to read or write the kernel's memory? What specific mechanisms protect the kernel memory?
        
        **The page table for Kernel's memory didn't set *the User bit(bit 2)*.——MMU compare current context's CS(cpl) with User bit in the pgtable for dest. address to confirm whether the executing instruction have the permission to access specific memory nor not.**
        
    3. What is the maximum amount of physical memory that this operating system can support? Why?
        
        **256MB. The reason is that KERNBASE=0xF0000000, only the memory above KERNBASE being available for OS kernel.**
        
    4. How much space overhead is there for managing memory, if we actually had the maximum amount of physical memory? How is this overhead broken down?
        
        **Page Tables=4KB+1024*4KB=4100KB≈4MB, struct pages=256MB/4KB*8B+4B(page_free_list header)≈512KB=0.5MB. The total space overhead is about up to 4.5MB——Using two-level paging can effectively decrease the space overhead when most of the pages are unused, usually 1MB less being used.**
        
        **(compared with direct paging which will always need 4GB/4KB*4B+0.5MB=4.5MB memory)**
        
    5. Revisit the page table setup in kern/entry.S and kern/entrypgdir.c. Immediately after we turn on paging, EIP is still a low number (a little over 1MB). At what point do we transition to running at an EIP above KERNBASE? What makes it possible for us to continue executing at a low EIP between when we enable paging and when we begin running at an EIP above KERNBASE? Why is this transition necessary?
        
        **The transition from low address to high address happens at:**
        
        ```c
        //entry.S
        mov	$relocated, %eax
        jmp	*%eax
        ```
        
        **Before the far jmp, PG has enabled. Since the boot-lifetime entry_pgdir mapped virtual address [0, 4MB) and [KERNBASE, KERNBASE+4MB) to physical address [0,4MB), low (vaddr) EIP can be executed. However, the [0,4MB) to [0,4MB) mapping is for back-compatible purpose, which should be cleaned in the future. The far jmp will make the [KERNBASE, KERNBASE+4MB) to [0,4MB) mapping active, so the legacy mapping can be disabled.**
        

## **Challenge**

1. PSE(4MB page) support: We consumed many physical pages to hold the page tables for the KERNBASE mapping. Do a more space-efficient job using the PTE_PS ("Page Size") bit in the page directory entries. This bit is supported on more recent x86 processors except old 80386. You will therefore have to refer to [Volume 3 of the current Intel manuals](https://pdos.csail.mit.edu/6.828/2018/readings/ia32/IA32-3A.pdf). Make sure you design the kernel to use this optimization only on processors that support it!
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%204.png)
    
    **Intel suggests that the processor maintains 4M-Byte page entries and 4K-Byte page entries in separate TLBs. So, placing often used code such as the kernel in a large page, frees up 4K-Byte-page TLB entries for application programs and tasks. (to reduce TLB misses and thus improve overall system performance)**
    
    **Firstly, we should confirm that the (V)CPU supports PS bit(CR4.PSE)——*cpuid* can show the features of (v)cpus. We implement a CPUID usage prototype based on inline-assembly *cpuid*(in *x86.h*) in *cpuid.c.*(omitted the implementation)**
    
    **Then in bootstrap, our *cpuid* function is executed before than *mem_init* to detect cpu's features. When cpu gets in *mem_init*, the PS-supporting representative variable *pse_support* will be initialized. If ture, *pse_mem_init* will run, otherwise *x32_mem_init* will be executed.**
    
    **To do it correctly and elegantly, we add some marcos in *memlayout.h* and *pmap.h*.**
    
    ```c
    //memlayout.h
    #define LPGSIZE PTSIZE
    
    //pmap.h
    #define page_alloc(alloc_flag) alloc_pages(1, alloc_flag)
    #define pse_page_alloc(alloc_flag) alloc_pages(1024, alloc_flag)
    #define	page_free(pp) free_pages(pp, 1)
    ```
    
    **Sine we have modified our physical memory management implementation (refer to below *LAB2-ext*), *pse_page_alloc* can be wrapped by the function *pages_alloc*. We only need to change the kernel mapping during *mem_init*, changing the loop stride to KPGSIZE and removing *pgdir_walk* call. (Of course, the mapping procedure should be varied when CPU doesn’t support PSE or PAE.) After mapping configured, we should enable PSE in CR4.**
    
    ```c
    //pmap.c
    extern unsigned int DFeInfo2;
    bool pae_support, pse_support;
    static void boot_map_pse_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm);
    
    void
    mem_init(void)
    {
    		pae_support = (DFeInfo2 & 0x00000040) >> 6;
    		pse_support = (DFeInfo2 & 0x00000008) >> 3;
    		...
    		if (pae_support && pse_support)
    			boot_map_pse_region(kern_pgdir, KERNBASE, ROUNDUP((~KERNBASE)+1,LPGSIZE), 0, PTE_W);
    		else
    			boot_map_region(kern_pgdir, KERNBASE, ~(KERNBASE)+1, 0, PTE_W);
    		...
    		if (pae_support && pse_support)
    			lcr4(rcr4()|CR4_PSE);
    		...
    }
    
    static void
    boot_map_pse_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm)
    {
    	// Fill this function in
    	size_t i;
    
    	if ((va & (LPGSIZE - 1)) || (pa & (LPGSIZE - 1)) || (size % LPGSIZE))
    		panic("non 4Mpage-aligned");
    
    	for (i = 0; i < size; i += LPGSIZE){
    		pgdir[PDX(va + i)] = 
    			(pde_t) ((pa + i) | perm | PTE_P | PTE_PS);		//permissions should be strict cos it's used as a pte
    	}
    }
    ```
    
    **Since the kernel page mapping has changed, the relevant check functions *check_kern_pgdir()* and *check_va2pa()* also need some modification.** 
    
    ```c
    static void
    check_kern_pgdir(void)
    {
    	...
    	// check phys mem
    	for (i = 0; i < npages * PGSIZE; i += LPGSIZE)
    		assert(check_va2pa(pgdir, KERNBASE + i) == i);
    	...
    }
    
    static physaddr_t
    check_va2pa(pde_t *pgdir, uintptr_t va)
    {
    	pte_t *p;
    	...
    	if (pae_support && pse_support) {
    		if (!(*pgdir & PTE_PS)) {
    			p = (pte_t*) KADDR(PTE_ADDR(*pgdir));
    			if (!(p[PTX(va)] & PTE_P))
    				return ~0;
    			return PTE_ADDR(p[PTX(va)]);
    		} else
    			return PTE_ADDR(*pgdir);	//pde is used as pte
    
    	} else {
    		//original implementation
    	}
    }
    ```
    
    **Result:** 
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%205.png)
    
    **Each PDE associated with kernel mem space now directly points to the corresponding physical memory. The lowest 12 bits of each PDE are equal to PSE | W | P . The kernel with modified memory management and PSE-pages can pass all the memory checks now.**
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%206.png)
    
2. Kernel monitor extension: Extend the JOS kernel monitor with commands to:
- Display in a useful and easy-to-read format all of the physical page mappings (or lack thereof) that apply to a particular range of virtual/linear addresses in the currently active address space. For example, you might enter '*showmap 0x3000 0x5000*' to display the page mappings and corresponding permission bits that apply to the pages at va 0x3000, 0x4000, and 0x5000.
- Explicitly set, clear, or change the permissions of any mapping in the current address space.
- Dump the contents of a range of memory given either a virtual or physical address range. Be sure the dump code behaves correctly when the range extends across page boundaries!
- Do anything else that you think might be useful later for debugging the kernel. (There's a good chance it will be!)
    
    **Declaration of new functions:** 
    
    ```c
    //monitor.h
    int mon_showmap(int argc, char **argv, struct Trapframe *tf);
    int mon_setperm(int argc, char **argv, struct Trapframe *tf);
    int mon_dumpmem(int argc, char **argv, struct Trapframe *tf);
    ```
    
    **Useful tools (functions and MARCOs):**
    
    ```c
    // map a capital character to permission bitnum
    static inline uint32_t
    char2perm(char c) {
    	switch (c)
    	{
    		case 'G': return PTE_G;		//0x100
    		case 'S': return PTE_PS;	//0x80
    		case 'D': return PTE_D;		//0x40
    		case 'A': return PTE_A;		//0x20
    		case 'C': return PTE_PCD;	//0x10
    		case 'T': return PTE_PWT;	//0x8
    		case 'U': return PTE_U;		//0x4
    		case 'W': return PTE_W;		//0x2
    		case 'P': return PTE_P;		//0x1
    		default: return -1;
    	}
    }
    
    static const char* perm_string = "PWUTCADSG";
    
    // permission number to a string of capital character
    static inline void
    perm2str(char* target, uint32_t perm) {
    	if (perm >= 0x200) {
    		warn("unexcepted permission!\n");
    		perm -= 0x200;
    	}
    
    	int i;
    	for (i = 8; i >= 0; i -= 1) {
    		if (perm & (1 << i)) {
    			*(target + 8 - i) = perm_string[i];
    		}
    		else
    			*(target + 8 - i) = '-';
    	}
    }
    
    // map a string of capital character to permission number
    static inline uint32_t
    str2perm(char* str){
    	uint32_t perm = 0;
    	while(*str)
    		perm |= char2perm(*str++);
    	if(perm < 0)
    		panic("Wrong perm-character\n");
    	// setting P bit should be forbidden
    	perm &= ~char2perm('P');
    	return perm;
    }
    
    #define PDE(pgdir,va) (pgdir[(PDX(va))])
    #define PTE_PTR(pgdir,va) (((pte_t*) KADDR(PTE_ADDR(PDE(pgdir,va)))) + PTX(va))
    #define PTE(pgdir,va) (*(PTE_PTR(pgdir,va)))
    #define PERM(entry) (entry & 0xFFF)
    ```
    
    **Implementation of new functions:**
    
    - **mon_showmap:**
        
        PSE_ENABLED PDE: va=va+LPGSIZE
        
        PSE_DISABLED PDE: va=va+PGSIZE
        
        ```c
        int
        mon_showmap(int argc, char **argv, struct Trapframe *tf)
        {
        	static const char *msg = 
            F_magenta"Usage: showmap <start> [<length>]\n"
        	"Output: VA:[VA], PA:[PA], PERM-bit:[GSDACTUWP]"ATTR_OFF"\n";
        
        	if (argc < 2 || ((argc == 2) && !isdigit(*argv[1])))
        		goto help;
        
        	uintptr_t vstart, vend;
            size_t vlen;
        	pde_t pde;
            pte_t pte;
        
            vstart = (uintptr_t)strtol(argv[1], 0, 0);
            vlen = argc >= 3 ? (size_t)strtol(argv[2], 0, 0) : 1;
            vend = vstart + vlen;
        
            vstart = ROUNDDOWN(vstart, PGSIZE);
        	cprintf(ATTR_bold);
            for(; vstart <= vend; ) {
        		//cprintf("vstart: 0x%08x, vend: 0x%08x\n", vstart, vend);
        		char permission[10];
        		permission[9] = '\0';
        		pde = PDE(kern_pgdir, vstart);
        		if (pde & PTE_P) {
        			if (pde & PTE_PS) {
        				physaddr_t pvaddr = PTE_ADDR(pde) | (PTX(vstart) << PTXSHIFT);
        				//cprintf("pde perm:0x%03x\n",PERM(pde));
        				perm2str(permission, PERM(pde));
        				cprintf("(PSE_ON) VA: 0x%08x, PA: 0x%08x, PERM: %s\n",
                    		vstart, pvaddr, permission);
        				vstart += LPGSIZE;
        			}
        			else {
        				pte = PTE(kern_pgdir, vstart);
        				if (pte & PTE_P) {
        					cprintf("pte perm:0x%03x\n",PERM(pte));
        					perm2str(permission, PERM(pte));
        					cprintf("(PSE_OFF) VA: 0x%08x, PA: 0x%08x, PERM: %s\n",
                    			vstart, PTE_ADDR(pde), permission);
        					vstart += PGSIZE;
        				}
        			}
                // pte = pgdir_walk(kern_pgdir, (void*)vstart, 0);
        		// if (pte && *pte & PTE_P) {
                //     cprintf("VA: 0x%08x, PA: 0x%08x, U-bit: %d, W-bit: %d, PS-bt: %d\n",
                //     vstart, PTE_ADDR(*pte), !!(*pte & PTE_U), !!(*pte & PTE_W), !!(*pte & PTE_PS));
                } else {
                    cprintf("VA: 0x%08x, PA: No Mapping\n", vstart);
        			cprintf(ATTR_OFF);
        			return -1;
        		}
            }
        	cprintf(ATTR_OFF);
            return 0;
        help:
        	cprintf(msg);
            return -1;
        }
        ```
        
        Result:
        
        ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%207.png)
        
    - **mon_setperm**:
        
        PSE_PDE: default set PS|P, change the lower 12 bits of PDE
        
        PTE: default set P, change the lower 12 bits of PTE
        
        ```c
        int 
        mon_setperm(int argc, char **argv, struct Trapframe *tf) 
        {
            static const char *msg = 
            F_magenta"Usage: setperm <virtual address> <permission>\n"
        	"*For PSE-enabled pgd, PTE_PS will be auto-set."ATTR_OFF"\n";
        
            if (argc != 3)
                goto help;
            
            uintptr_t va;
            uint16_t perm;
        	pde_t* pde;
            pte_t *pte;
        
            va = (uintptr_t)strtol(argv[1], 0, 0);
            perm = (uint16_t)strtol(argv[2], 0, 0);
        	char permission[10];
        	permission[9]='\0';
        	strncpy(permission, argv[2], 9);
        	perm = str2perm(permission);
        	//cprintf("perm=0x%03x\n", perm);
        	pde = &kern_pgdir[PDX(va)];
        	if (*pde & PTE_PS){
        		if (*pde & PTE_P) {
        			*pde = (*pde & ~0xFFF) | perm | PTE_P | PTE_PS;
        			cprintf("New mapping = VA: 0x%08x, PA: 0x%08x, perm: 0x%03x.\n",
        				va, PTE_ADDR(*pde)|(PTX(va) << PTXSHIFT)|PGOFF(va), PERM(*pde));
        		}
        		else {
        			cprintf("No such mapping\n");
        			return -1;
        		}
        	} else {
        		pte = pgdir_walk(kern_pgdir, (void*)va, 0);
        		if (pte && *pte & PTE_P) {
        			*pte = (*pte & ~0xFFF) | (perm & 0xFFF) | PTE_P;
        			cprintf("New mapping = VA: 0x%08x, PA: 0x%08x, perm: 0x%03x.\n",
        				va, PTE_ADDR(*pte)|PGOFF(va), PERM(*pte));
        		} else {
        			cprintf("No such mapping\n");
        			return -1;
        		}
        	}
            return 0;
        help: 
            cprintf(msg);
            return -1;
        }
        ```
        
        Result:
        
        ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%208.png)
        
    - **mon_dumpmem**:
        
        [-v, --virtual]: dump a range of virtual memory
        
        -p, --physical: dump a range of physical memory
        
        step: 1
        
        ```c
        int
        mon_dumpmem(int argc, char **argv, struct Trapframe *tf)
        {
        	static const char *msg =
            F_magenta"Usage: dumpmem [option] <start> <length>\n"
            "\t-p, --physical\tuse physical address\n"
        	"\t[-v, --virtual]\tuse virtual address(default)"ATTR_OFF"\n";
        
        	int recog = 0;
        	int phys = 0;
        	if (argc == 4) {
                int i;
                for (i = 1; i < argc; ++i) {
                    if (!strcmp(argv[i], "-p") || !strcmp(argv[i], "--physical")) {
        				recog = 1;
                        phys = 1;
                        break;
                    }
        			else if (!strcmp(argv[i], "-v") || !strcmp(argv[i], "--virtual")) {
        				recog = 1;
                        break;
                    }
                }
                if (!recog)
                    goto help;
                for (int j = i; j < argc - 1; ++j)
                    argv[j] = argv[j + 1];
            } else if (argc != 3) {
                goto help;
            }
        
        	uint32_t mstart, mend;
            size_t mlen;
            
            mstart = (uint32_t)strtol(argv[1], 0, 0);
            mlen = (size_t)strtol(argv[2], 0, 0);
            mend = mstart + mlen;
        
        	cprintf(ATTR_bold);
            if (phys) {
                if (mend > (~KERNBASE + 1)) {
                    cprintf("Target memory out of range\n"
        			"Only dump to TOP.\n");
        			mend = ~KERNBASE + 1;
                }
                for (; mstart < mend; ++mstart) {
                    cprintf("[PA 0x%08x]: %02x\n", mstart, *(uint8_t*)KADDR(mstart));
                }
            } else {
                uint32_t next;
                pte_t *pte;
                while(mstart < mend) {
        			if (PDE(kern_pgdir, mstart) & PTE_PS) {
        				if (PDE(kern_pgdir, mstart) & PTE_P) {
        					next = MIN((uint32_t)PGADDR(PDX(mstart), PTX(mstart) + 1, 0), mend);
        					for (; mstart < next; ++mstart)
        						cprintf("[VA 0x%08x, PA 0x%08x]: %02x\n",
        						 mstart, PTE_ADDR(PDE(kern_pgdir, mstart))|(PTX(mstart) << PTXSHIFT)|PGOFF(mstart), *(uint8_t*)mstart);
        				} else {
        					cprintf("[VA 0x%08x, PA No-mapping]: None\n", mstart);
        
        				}
        			} else {
        				if (!(pte = pgdir_walk(kern_pgdir, (void*)mstart, 0))) {
        					next = MIN((uint32_t)PGADDR(PDX(mstart) + 1, 0, 0), mend);
        					for (; mstart < next; ++mstart)
        						cprintf("[VA 0x%08x, PA No-mapping]: None\n", mstart);
        				} else if (!(*pte & PTE_P)) {
        					next = MIN((uint32_t)PGADDR(PDX(mstart), PTX(mstart) + 1, 0), mend);
        					for (; mstart < next; ++mstart)
        						cprintf("[VA 0x%08x, PA No-mapping]: None\n", mstart);
        				} else {
        					next = MIN((uint32_t)PGADDR(PDX(mstart), PTX(mstart) + 1, 0), mend);
        					for (; mstart < next; ++mstart)
        						cprintf("[VA 0x%08x, PA 0x%08x]: %02x\n", mstart, PTE_ADDR(*pte) | PGOFF(mstart), *(uint8_t*)mstart);
        				}
        			}
                }
            }
        	cprintf(ATTR_OFF);
            return 0;
        help:
        	cprintf(msg);
        	return -1;
        }
        ```
        
        Result:
        
        ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%209.png)
        
1. **Place Kernel in a seperate pgtable*: Each user-level environment maps the kernel. Change JOS so that the kernel has its own page table and so that a user-level environment runs with a minimal number of kernel pages mapped (each user-level environment maps just enough pages mapped so that the user-level environment can enter and leave the kernel correctly) . You also have to come up with a plan for the kernel to read/write arguments to system calls.
    
    **Design: (refer to Linux KPTI) The user-space space only contains a minimal set of kernel-space handling system calls and interrupts. The handlers eventually call task swtich (cr3 swtich) instruction to execute in kernel space, which mapped in a separate page tables.** 
    
    ![Untitled](/images/posts/Lab2-Memory-Management/Untitled%2010.png)
    
    **Disadvantage (Performance overhead) : Before each interrupt, syscall and exception’s entry and exit should change cr3 to kernel pgtable, which will take several hundred clock cycles. More TLB flush during syscalls results from CR3 switch.**
    
2. **follow the bouncing kernel*: Write up an outline of how a kernel could be designed to allow user environments unrestricted use of the full 4GB virtual and linear address space. Hint: do the previous challenge exercise first, which reduces the kernel to a few mappings in a user environment. Hint: the technique is sometimes known as "*follow the bouncing kernel*." In your design, be sure to address exactly what has to happen when the processor transitions between kernel and user modes, and how the kernel would accomplish such transitions. Also describe how the kernel would access physical memory and I/O devices in this scheme, and how the kernel would access a user environment's virtual address space during system calls and the like. Finally, think about and describe the advantages and disadvantages of such a scheme in terms of flexibility, performance, kernel complexity, and other factors you can think of.
    
    (Answer in the future)
    
3. Buddy System: Since JOS kernel's memory management only allocates and frees memory on page granularity, we do not have anything comparable to a general-purpose `malloc` / `free` facility that we can use within the kernel. This could be a problem if we want to support certain types of I/O devices that require *physically contiguous* buffers larger than 4KB in size, or if we want user-level environments, and not just the kernel, to be able to allocate and map 4MB *superpages* for maximum processor efficiency. (See the earlier challenge **PSE support**.)
    
    Generalize the kernel's memory allocation system to support pages of a variety of **power-of-two** allocation unit sizes from 4KB up to some reasonable maximum of your choice. Be sure you have some way to divide larger allocation units into smaller ones on demand, and to coalesce multiple small allocation units back into larger units when possible. 
    
    **Since the kernel page is arranged in a 4K form, our buddy system will allocate a memory unit from 4KB to (2^10)1024*4KB=4MB to simply our implementation.** 
    
    ```c
    //pmap.h
    #define MAX_ORDER 10
    ```
    

### Special Modification in lab2:

[Lab2-extension](2023-04-21-Lab2-ext.md)